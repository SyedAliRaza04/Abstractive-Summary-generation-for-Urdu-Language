{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee0f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urduhack\n",
    "import numpy as np\n",
    "from urduhack import CoNLL\n",
    "from urduhack.preprocessing import normalize_whitespace, remove_punctuation, remove_accents, replace_urls, replace_numbers, replace_currency_symbols, remove_english_alphabets\n",
    "from urduhack.preprocessing import replace_emails\n",
    "from urduhack.normalization import remove_diacritics\n",
    "from urduhack.preprocessing import remove_accents\n",
    "from urduhack.preprocessing import replace_urls\n",
    "\n",
    "from urduhack.conll.tests.test_parser import CONLL_SENTENCE\n",
    "from urduhack.tokenization import word_tokenizer\n",
    "from urduhack.tokenization import sentence_tokenizer\n",
    "from urduhack.normalization import normalize\n",
    "# urduhack.download()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from transformers import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b36143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وفاقی کابینہ نے قومی احتساب بیورو (نیب) قوانین میں ترامیم کے لیے وزیر قانون کی سربراہی میں کمیٹی تشکیل دینے کی منظوری دے دی۔ \n",
      "\n",
      "\n",
      "\n",
      "وزیراعظم شہباز شریف کی زیرِ صدارت وفاقی کابینہ اجلاس کا اعلامیہ جاری کردیا گیا ہے۔ \n",
      "\n",
      "\n",
      "\n",
      "اجلاس کے دوران وزیراعظم شہباز شریف نے گفتگو کرتے ہوئے کہا کہ ملک شدید گرمی کی لپیٹ میں ہے۔\n",
      "\n",
      "\n",
      "\n",
      "وزیراعظم نے کہا کہ خصوصی ٹاسک فورس وزارت ماحولیاتی تبدیلی کے زیرِ انتظام تشکیل دی گئی ہے، ٹاسک فورس ماحولیاتی تبدیلیوں کے تدارک کے لیے اقدامات کرے گی۔\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "حکومت کا چیئرمین نیب جاوید اقبال کو ہٹانے کا فیصلہ\n",
      "\n",
      "\n",
      "\n",
      "اعلامیے کے مطابق اس اجلاس میں نیب ترامیم کے حوالے سے تفصیلی گفتگو  ہوئی، جبکہ کابینہ نے نیب ترامیم کے لیے وزیر قانون کی سربراہی میں کمیٹی تشکیل دینے کی منظوری دی۔\n",
      "\n",
      "\n",
      "\n",
      "اس کمیٹی میں وکالت، بینکاری اور دیگر شعبوں سے منسلک نامور شخصیات شامل کی جائیں گی۔ \n",
      "\n",
      "\n",
      "\n",
      "کابینہ اراکین نے کہا کہ نیب کا کالا قانون سیاسی انتقام، کاروباری طبقے کو ہراساں کرنے کے لیے استعمال ہوتا رہا، نیب کے کالے قانون کی وجہ سے بیوروکریسی فیصلے لینے سے ڈرتی ہے۔\n",
      "\n",
      "\n",
      "\n",
      "دوسری جانب وفاقی کابینہ نے سول سرونٹس (ڈائرکٹری ریٹائرمنٹ فار سروس) رولز 2020 منسوخ کرنے کی منظوری دے دی۔\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "پٹرول سبسڈی جاری رکھنا غلط، ڈیفالٹ کا خطرہ نہیں، سخت فیصلے کرنا ہونگے، معاشی ماہرین\n",
      "\n",
      "\n",
      "\n",
      "کابینہ نے ان رولز کے تحت سرکاری افسران کے خلاف شروع کی گئی کارروائیوں کو ختم کرنے کی بھی منظوری دی۔\n",
      "\n",
      "\n",
      "\n",
      "اعلامیے کے مطابق کابینہ نے 75ویں یوم آزادی اور اسٹیٹ بینک کے قیام کی مناسبت سے یادگاری بینک نوٹ کے ڈیزائن کی منظوری دی۔\n",
      "\n",
      "\n",
      "\n",
      "وزارت خزانہ نے بتایا کہ بینک نوٹ بین الاقوامی پرنٹنگ کمپنیوں کے ذریعے چھاپے جائیں، 6.64 ملین ڈالر خرچہ آئے گا۔\n",
      "\n",
      "\n",
      "\n",
      "کابینہ اراکین نے اپنی رائے دیتے ہوئے کہا کہ بینک نوٹ پاکستان کے اندر ہی چھاپے جائیں گے تاکہ قوم کا قیمتی پیسہ بچایا جا سکے۔\n",
      "\n",
      "\n",
      "\n",
      "اعلامیے کے مطابق وزارت کامرس نے کابینہ کو برآمدات، درآمدات اور تجارت کے توازن کے حوالے سے تفصیلی تجزیہ پیش کیا۔\n",
      "\n",
      "\n",
      "\n",
      "یہ بھی پڑھیے\n",
      "\n",
      "اللّٰہ کی شان ہے اسد عمر معیشت پر باتیں کر رہے ہیں‘ مفتاح\n",
      "\n",
      "پاکستان کو آئندہ مالی سال دیوالیہ ہونے کا سخت خطرہ لاحق، شبر زیدی\n",
      "\n",
      "بریفنگ کے دوران بتایا گیا کہ مالی سال 22-2021ء کے پہلے 10 ماہ میں برآمدات کا حجم 31.2 ارب ڈالر جبکہ درآمدات کا حجم 76.7 ارب ڈالر رہا۔\n",
      "\n",
      "\n",
      "\n",
      "اراکین کو بریفنگ کے دوران بتایا گیا کہ مالی سال 22-2021ء کے پہلے 10 ماہ میں برآمدات میں 4.95 ارب ڈالر جبکہ درآمدات میں 11.16 ارب ڈالر کا اضافہ ریکارڈ کیا گیا۔ \n",
      "\n",
      "\n",
      "\n",
      "بریفنگ میں یہ بھی بتایا گیا کہ مالی سال 21-2020ء کے جولائی تا اپریل اور مالی سال 22-2021ء کے اسی دورانیے میں برآمدات کے حجم میں 25.6 فیصد اضافہ دیکھا گیا جبکہ درآمدات میں 46.5 فیصد اضافہ ریکارڈ ہوا۔ اسی دورانیے میں ٹریڈ بیلنس میں 64.9 فیصد کا اضافہ ہوا۔ \n",
      "\n",
      "\n",
      "\n",
      "اعلامیے کے مطابق کابینہ نے ایگروبیسڈ صنعت کے فروغ، فصلوں سے پیداوار بڑھانے خصوصی پالیسی ساز کمیٹی تشکیل دینے کی منظوری دی۔ \n",
      "\n",
      "\n",
      "\n",
      "اس کمیٹی میں وزیر تجارت، وزیر صنعت و پیداوار، وزیر نیشنل فوڈ سیکورٹی، ڈویژنز کے سیکرٹریز شامل ہوں گے۔ \n",
      "\n",
      "\n",
      "\n",
      "وزیراعظم نے ہدایت کی ہے کہ اس کمیٹی کا اجلاس آج ہی بلایا جائے۔\n",
      "\n",
      "\n",
      "\n",
      "اعلامیے کے مطابق وفاقی کابینہ نے اقتصادی رابطہ کمیٹی کے 16 مئی کو اجلاس کے فیصلوں کی توثیق دی۔ \n"
     ]
    }
   ],
   "source": [
    "# spliting the articles \n",
    "#into sentences\n",
    "article = open('urduarticle.txt' , encoding = 'utf-8')\n",
    "sentences = []\n",
    "\n",
    "# text = \n",
    "# tokenizing senytences with sentence_tokenizer()\n",
    "for sent in article:\n",
    "    print(sent)\n",
    "    sentences.append(sentence_tokenizer(sent)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "343e7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"urduarticle.txt\", 'r', encoding='utf-8') as f:\n",
    "    texts = f.read().splitlines()\n",
    "\n",
    "\n",
    "inputs = list(zip(texts, texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c231406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_summary = ['وزیراعظم شہباز شریف کی زیرِ صدارت وفاقی کابینہ اجلاس کا اعلامیہ جاری کردیا گیا ہے۔',\n",
    " 'وزیراعظم نے ہدایت کی ہے کہ اس کمیٹی کا اجلاس آج ہی بلایا جائے۔','اسی دورانیے میں ٹریڈ بیلنس میں 64.9 فیصد کا اضافہ ہوا۔',\n",
    " 'اعلامیے کے مطابق وفاقی کابینہ نے اقتصادی رابطہ کمیٹی کے 16 مئی کو اجلاس کے فیصلوں کی توثیق دی۔',\n",
    " 'بریفنگ کے دوران بتایا گیا کہ مالی سال 22-2021ء کے پہلے 10 ماہ میں برآمدات کا حجم 31.2 ارب ڈالر جبکہ درآمدات کا حجم 76.7 ارب ڈالر رہا۔',\n",
    " 'اراکین کو بریفنگ کے دوران بتایا گیا کہ مالی سال 22-2021ء کے پہلے 10 ماہ میں برآمدات میں 4.95 ارب ڈالر جبکہ درآمدات میں 11.16 ارب ڈالر کا اضافہ ریکارڈ کیا گیا۔',\n",
    " 'اسد عمر معیشت پر باتیں کر رہے ہیں‘ مفتاح' ,\n",
    "        ]\n",
    "len(reference_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83121ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['وفاقی کابینہ نے قومی احتساب بیورو (نیب) قوانین میں ترامیم کے لیے وزیر قانون کی سربراہی میں کمیٹی تشکیل دینے کی منظوری دے دی۔', 'وزیراعظم شہباز شریف کی زیرِ صدارت وفاقی کابینہ اجلاس کا اعلامیہ جاری کردیا گیا ہے۔', 'اجلاس کے دوران وزیراعظم شہباز شریف نے گفتگو کرتے ہوئے کہا کہ ملک شدید گرمی کی لپیٹ میں ہے۔', 'وزیراعظم نے کہا کہ خصوصی ٹاسک فورس وزارت ماحولیاتی تبدیلی کے زیرِ انتظام تشکیل دی گئی ہے، ٹاسک فورس ماحولیاتی تبدیلیوں کے تدارک کے لیے اقدامات کرے گی۔', 'حکومت کا چیئرمین نیب جاوید اقبال کو ہٹانے کا فیصلہ', 'اعلامیے کے مطابق اس اجلاس میں نیب ترامیم کے حوالے سے تفصیلی گفتگو ہوئی، جبکہ کابینہ نے نیب ترامیم کے لیے وزیر قانون کی سربراہی میں کمیٹی تشکیل دینے کی منظوری دی۔', 'اس کمیٹی میں وکالت، بینکاری اور دیگر شعبوں سے منسلک نامور شخصیات شامل کی جائیں گی۔', 'کابینہ اراکین نے کہا کہ نیب کا کالا قانون سیاسی انتقام، کاروباری طبقے کو ہراساں کرنے کے لیے استعمال ہوتا رہا، نیب کے کالے قانون کی وجہ سے بیوروکریسی فیصلے لینے سے ڈرتی ہے۔', 'دوسری جانب وفاقی کابینہ نے سول سرونٹس (ڈائرکٹری ریٹائرمنٹ فار سروس) رولز 2020 منسوخ کرنے کی منظوری دے دی۔', 'پٹرول سبسڈی جاری رکھنا غلط، ڈیفالٹ کا خطرہ نہیں، سخت فیصلے کرنا ہونگے، معاشی ماہرین', 'کابینہ نے ان رولز کے تحت سرکاری افسران کے خلاف شروع کی گئی کارروائیوں کو ختم کرنے کی بھی منظوری دی۔', 'اعلامیے کے مطابق کابینہ نے 75ویں یوم آزادی اور اسٹیٹ بینک کے قیام کی مناسبت سے یادگاری بینک نوٹ کے ڈیزائن کی منظوری دی۔', 'وزارت خزانہ نے بتایا کہ بینک نوٹ بین الاقوامی پرنٹنگ کمپنیوں کے ذریعے چھاپے جائیں، 6.64 ملین ڈالر خرچہ آئے گا۔', 'کابینہ اراکین نے اپنی رائے دیتے ہوئے کہا کہ بینک نوٹ پاکستان کے اندر ہی چھاپے جائیں گے', 'تاکہ قوم کا قیمتی پیسہ بچایا جا سکے۔', 'اعلامیے کے مطابق وزارت کامرس نے کابینہ کو برآمدات، درآمدات اور تجارت کے توازن کے حوالے سے تفصیلی تجزیہ پیش کیا۔', 'یہ بھی پڑھیے', 'اللّٰہ کی شان ہے', 'اسد عمر معیشت پر باتیں کر رہے ہیں‘ مفتاح', 'پاکستان کو آئندہ مالی سال دیوالیہ ہونے کا سخت خطرہ لاحق، شبر زیدی', 'بریفنگ کے دوران بتایا گیا کہ مالی سال 22-2021ء کے پہلے 10 ماہ میں برآمدات کا حجم 31.2 ارب ڈالر جبکہ درآمدات کا حجم 76.7 ارب ڈالر رہا۔', 'اراکین کو بریفنگ کے دوران بتایا گیا کہ مالی سال 22-2021ء کے پہلے 10 ماہ میں برآمدات میں 4.95 ارب ڈالر جبکہ درآمدات میں 11.16 ارب ڈالر کا اضافہ ریکارڈ کیا گیا۔', 'بریفنگ میں یہ بھی بتایا گیا کہ مالی سال 21-2020ء کے جولائی تا اپریل اور مالی سال 22-2021ء کے اسی دورانیے میں برآمدات کے حجم میں 25.6 فیصد اضافہ دیکھا گیا جبکہ درآمدات میں 46.5 فیصد اضافہ ریکارڈ ہوا۔', 'اسی دورانیے میں ٹریڈ بیلنس میں 64.9 فیصد کا اضافہ ہوا۔', 'اعلامیے کے مطابق کابینہ نے ایگروبیسڈ صنعت کے فروغ، فصلوں سے پیداوار بڑھانے خصوصی پالیسی ساز کمیٹی تشکیل دینے کی منظوری دی۔', 'اس کمیٹی میں وزیر تجارت، وزیر صنعت و پیداوار، وزیر نیشنل فوڈ سیکورٹی، ڈویژنز کے سیکرٹریز شامل ہوں', 'وزیراعظم نے ہدایت کی ہے کہ اس کمیٹی کا اجلاس آج ہی بلایا جائے۔', 'اعلامیے کے مطابق وفاقی کابینہ نے اقتصادی رابطہ کمیٹی کے 16 مئی کو اجلاس کے فیصلوں کی توثیق دی۔']\n"
     ]
    }
   ],
   "source": [
    "# A Sample class with init method\n",
    "sentences = []\n",
    "sent  = []\n",
    "class Prep_text:\n",
    "# init method or constructor\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "        self.sentences = None\n",
    "        self.clean_sentences = []\n",
    "        self.rem_stop_words = []\n",
    "        self.lemmatized_words = []\n",
    "        \n",
    "    #file handling\n",
    "    \n",
    "    def file_handling(self , articlee):\n",
    "        articlee = open('urduarticle.txt' , encoding = 'utf-8')\n",
    "        self.article = articlee\n",
    "        \n",
    "     # urdu hack tokinizing \n",
    "        \n",
    "    def Tokenizer(self):\n",
    "        for sent in self.article:\n",
    "            sentences.append(sentence_tokenizer(sent)) \n",
    "        self.sentences = sentences\n",
    "        \n",
    "    # convert any dimentional array to only a single dimentional array\n",
    "    \n",
    "    def single_dim(self):\n",
    "        for x in self.sentences:\n",
    "            for y in x:\n",
    "                sent.append(y)\n",
    "        self.sentences = sent\n",
    "        \n",
    "        #remove punctuation marks and special chqaraters\n",
    "        \n",
    "    def cleansentences(self):\n",
    "#         clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "#         self.sentences = [s.lower() for s in self.sentences]\n",
    "        for sent in self.sentences:\n",
    "            sent = sent.lower()\n",
    "            self.clean_sentences.append(sent)\n",
    "#         return\n",
    "\n",
    "    #remove the dia critics\n",
    "    def diacritics(self):\n",
    "        normalized_text = remove_diacritics(self.sentences)\n",
    "\n",
    "    def accents_remove(self):\n",
    "        self.sentences = []\n",
    "        self.sentences  = remove_accents(self.clean_sentences)\n",
    "\n",
    "        #remove the urls from the tetx\n",
    "    def urls_remove(self):\n",
    "        clean_sentences = replace_urls(self.clean_sentences)#\n",
    "\n",
    "    \n",
    "    def rem_stopwords(self):\n",
    "        sw = open('kg_uswords.txt', encoding='utf-8' )#.decode('utf-8')   \n",
    "        flines = sw.readlines()\n",
    "        swords = []\n",
    "#         rem = [] \n",
    "        for word in words:\n",
    "            if word not in swords:\n",
    "                self.rem_stop_words.append(word)\n",
    "    def lemmetization(self):\n",
    "        words = []\n",
    "        lemmatized_words = []\n",
    "        for sent in doc.sentences:\n",
    "            print(sent.text)\n",
    "#             print('\\n************************')\n",
    "        for word in sent.words:\n",
    "            words.append(word.text)\n",
    "            lemmatized_words.append(word.lemma)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "p = Prep_text(article)\n",
    "p.file_handling(article)\n",
    "p.Tokenizer()\n",
    "p.single_dim()\n",
    "p.cleansentences()\n",
    "# p.urls_remove()\n",
    "# p.accents_remove()\n",
    "# p.cleansentences()\n",
    "# print(p.sentences)\n",
    "print(p.clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1520d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['وفاقی', 'کابینہ', 'نے', 'قومی', 'احتساب', 'بیورو', '(نیب)', 'قوانین', 'میں', 'ترامیم', 'کے', 'لیے', 'وزیر', 'قانون', 'کی', 'سربراہی', 'میں', 'کمیٹی', 'تشکیل', 'دینے', 'کی', 'منظوری', 'دے', 'دی۔', 'وزیراعظم', 'شہباز', 'شریف', 'کی', 'زیرِ', 'صدارت', 'وفاقی', 'کابینہ', 'اجلاس', 'کا', 'اعلامیہ', 'جاری', 'کردیا', 'گیا', 'ہے۔', 'اجلاس', 'کے', 'دوران', 'وزیراعظم', 'شہباز', 'شریف', 'نے', 'گفتگو', 'کرتے', 'ہوئے', 'کہا', 'کہ', 'ملک', 'شدید', 'گرمی', 'کی', 'لپیٹ', 'میں', 'ہے۔', 'وزیراعظم', 'نے', 'کہا', 'کہ', 'خصوصی', 'ٹاسک', 'فورس', 'وزارت', 'ماحولیاتی', 'تبدیلی', 'کے', 'زیرِ', 'انتظام', 'تشکیل', 'دی', 'گئی', 'ہے،', 'ٹاسک', 'فورس', 'ماحولیاتی', 'تبدیلیوں', 'کے', 'تدارک', 'کے', 'لیے', 'اقدامات', 'کرے', 'گی۔', 'حکومت', 'کا', 'چیئرمین', 'نیب', 'جاوید', 'اقبال', 'کو', 'ہٹانے', 'کا', 'فیصلہ', 'اعلامیے', 'کے', 'مطابق', 'اس', 'اجلاس', 'میں', 'نیب', 'ترامیم', 'کے', 'حوالے', 'سے', 'تفصیلی', 'گفتگو', 'ہوئی،', 'جبکہ', 'کابینہ', 'نے', 'نیب', 'ترامیم', 'کے', 'لیے', 'وزیر', 'قانون', 'کی', 'سربراہی', 'میں', 'کمیٹی', 'تشکیل', 'دینے', 'کی', 'منظوری', 'دی۔', 'اس', 'کمیٹی', 'میں', 'وکالت،', 'بینکاری', 'اور', 'دیگر', 'شعبوں', 'سے', 'منسلک', 'نامور', 'شخصیات', 'شامل', 'کی', 'جائیں', 'گی۔', 'کابینہ', 'اراکین', 'نے', 'کہا', 'کہ', 'نیب', 'کا', 'کالا', 'قانون', 'سیاسی', 'انتقام،', 'کاروباری', 'طبقے', 'کو', 'ہراساں', 'کرنے', 'کے', 'لیے', 'استعمال', 'ہوتا', 'رہا،', 'نیب', 'کے', 'کالے', 'قانون', 'کی', 'وجہ', 'سے', 'بیوروکریسی', 'فیصلے', 'لینے', 'سے', 'ڈرتی', 'ہے۔', 'دوسری', 'جانب', 'وفاقی', 'کابینہ', 'نے', 'سول', 'سرونٹس', '(ڈائرکٹری', 'ریٹائرمنٹ', 'فار', 'سروس)', 'رولز', '2020', 'منسوخ', 'کرنے', 'کی', 'منظوری', 'دے', 'دی۔', 'پٹرول', 'سبسڈی', 'جاری', 'رکھنا', 'غلط،', 'ڈیفالٹ', 'کا', 'خطرہ', 'نہیں،', 'سخت', 'فیصلے', 'کرنا', 'ہونگے،', 'معاشی', 'ماہرین', 'کابینہ', 'نے', 'ان', 'رولز', 'کے', 'تحت', 'سرکاری', 'افسران', 'کے', 'خلاف', 'شروع', 'کی', 'گئی', 'کارروائیوں', 'کو', 'ختم', 'کرنے', 'کی', 'بھی', 'منظوری', 'دی۔', 'اعلامیے', 'کے', 'مطابق', 'کابینہ', 'نے', '75ویں', 'یوم', 'آزادی', 'اور', 'اسٹیٹ', 'بینک', 'کے', 'قیام', 'کی', 'مناسبت', 'سے', 'یادگاری', 'بینک', 'نوٹ', 'کے', 'ڈیزائن', 'کی', 'منظوری', 'دی۔', 'وزارت', 'خزانہ', 'نے', 'بتایا', 'کہ', 'بینک', 'نوٹ', 'بین', 'الاقوامی', 'پرنٹنگ', 'کمپنیوں', 'کے', 'ذریعے', 'چھاپے', 'جائیں،', '6.64', 'ملین', 'ڈالر', 'خرچہ', 'آئے', 'گا۔', 'کابینہ', 'اراکین', 'نے', 'اپنی', 'رائے', 'دیتے', 'ہوئے', 'کہا', 'کہ', 'بینک', 'نوٹ', 'پاکستان', 'کے', 'اندر', 'ہی', 'چھاپے', 'جائیں', 'گے', 'تاکہ', 'قوم', 'کا', 'قیمتی', 'پیسہ', 'بچایا', 'جا', 'سکے۔', 'اعلامیے', 'کے', 'مطابق', 'وزارت', 'کامرس', 'نے', 'کابینہ', 'کو', 'برآمدات،', 'درآمدات', 'اور', 'تجارت', 'کے', 'توازن', 'کے', 'حوالے', 'سے', 'تفصیلی', 'تجزیہ', 'پیش', 'کیا۔', 'یہ', 'بھی', 'پڑھیے', 'اللّٰہ', 'کی', 'شان', 'ہے', 'اسد', 'عمر', 'معیشت', 'پر', 'باتیں', 'کر', 'رہے', 'ہیں‘', 'مفتاح', 'پاکستان', 'کو', 'آئندہ', 'مالی', 'سال', 'دیوالیہ', 'ہونے', 'کا', 'سخت', 'خطرہ', 'لاحق،', 'شبر', 'زیدی', 'بریفنگ', 'کے', 'دوران', 'بتایا', 'گیا', 'کہ', 'مالی', 'سال', '22-2021ء', 'کے', 'پہلے', '10', 'ماہ', 'میں', 'برآمدات', 'کا', 'حجم', '31.2', 'ارب', 'ڈالر', 'جبکہ', 'درآمدات', 'کا', 'حجم', '76.7', 'ارب', 'ڈالر', 'رہا۔', 'اراکین', 'کو', 'بریفنگ', 'کے', 'دوران', 'بتایا', 'گیا', 'کہ', 'مالی', 'سال', '22-2021ء', 'کے', 'پہلے', '10', 'ماہ', 'میں', 'برآمدات', 'میں', '4.95', 'ارب', 'ڈالر', 'جبکہ', 'درآمدات', 'میں', '11.16', 'ارب', 'ڈالر', 'کا', 'اضافہ', 'ریکارڈ', 'کیا', 'گیا۔', 'بریفنگ', 'میں', 'یہ', 'بھی', 'بتایا', 'گیا', 'کہ', 'مالی', 'سال', '21-2020ء', 'کے', 'جولائی', 'تا', 'اپریل', 'اور', 'مالی', 'سال', '22-2021ء', 'کے', 'اسی', 'دورانیے', 'میں', 'برآمدات', 'کے', 'حجم', 'میں', '25.6', 'فیصد', 'اضافہ', 'دیکھا', 'گیا', 'جبکہ', 'درآمدات', 'میں', '46.5', 'فیصد', 'اضافہ', 'ریکارڈ', 'ہوا۔', 'اسی', 'دورانیے', 'میں', 'ٹریڈ', 'بیلنس', 'میں', '64.9', 'فیصد', 'کا', 'اضافہ', 'ہوا۔', 'اعلامیے', 'کے', 'مطابق', 'کابینہ', 'نے', 'ایگروبیسڈ', 'صنعت', 'کے', 'فروغ،', 'فصلوں', 'سے', 'پیداوار', 'بڑھانے', 'خصوصی', 'پالیسی', 'ساز', 'کمیٹی', 'تشکیل', 'دینے', 'کی', 'منظوری', 'دی۔', 'اس', 'کمیٹی', 'میں', 'وزیر', 'تجارت،', 'وزیر', 'صنعت', 'و', 'پیداوار،', 'وزیر', 'نیشنل', 'فوڈ', 'سیکورٹی،', 'ڈویژنز', 'کے', 'سیکرٹریز', 'شامل', 'ہوں', 'وزیراعظم', 'نے', 'ہدایت', 'کی', 'ہے', 'کہ', 'اس', 'کمیٹی', 'کا', 'اجلاس', 'آج', 'ہی', 'بلایا', 'جائے۔', 'اعلامیے', 'کے', 'مطابق', 'وفاقی', 'کابینہ', 'نے', 'اقتصادی', 'رابطہ', 'کمیٹی', 'کے', '16', 'مئی', 'کو', 'اجلاس', 'کے', 'فیصلوں', 'کی', 'توثیق', 'دی۔']\n"
     ]
    }
   ],
   "source": [
    "text  = p.clean_sentences.copy()\n",
    "type(text)\n",
    "processed_text = \" \".join(text)\n",
    "processed_text = processed_text.split()\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ac125b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
    "#bert = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Encode the input and output sequences\n",
    "input_ids = tokenizer.batch_encode_plus(inputs, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "summary_ids = tokenizer.batch_encode_plus(reference_summary, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# Convert the input and output sequences into a tuple\n",
    "input_sequence = (input_ids['input_ids'], input_ids['attention_mask'])\n",
    "output_sequence = (summary_ids['input_ids'], summary_ids['attention_mask'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_input, val_input, train_output, val_output = train_test_split(input_sequence, output_sequence, test_size=0.2, random_state=42)\n",
    "# Define the model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "max_summary_length = 64\n",
    "batch_size = 32\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_state=True)\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        # Reshape the input to remove the extra dimension\n",
    "        x = tf.squeeze(x, axis=0)\n",
    "        _, state_h, state_c = self.lstm(x)\n",
    "        state = [state_h, state_c]\n",
    "        return state\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "\n",
    "    def call(self, input_ids, encoder_state):\n",
    "        x = self.embedding(input_ids)\n",
    "        lstm_output, state_h, state_c = self.lstm(x, initial_state=encoder_state)\n",
    "        context_vector, attention_weights = self.attention([lstm_output, encoder_state])\n",
    "        output = self.dense(context_vector)\n",
    "        return output, state_h, attention_weights\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_ids, summary_texts, encoder, decoder, optimizer, loss_fn):\n",
    "    loss = 0\n",
    "\n",
    "    # Convert input_ids to TensorFlow tensor\n",
    "    input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "\n",
    "    # Tokenize and encode the summary texts\n",
    "    # summary_encodings = tokenizer(summary_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "    summary_encodings = tokenizer(summary_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "    print(type(summary_texts))\n",
    "    # Extract the IDs from the summary_encodings\n",
    "    summary_ids = summary_encodings[\"input_ids\"]\n",
    "\n",
    "    # Encode the input\n",
    "    encoder_state = encoder(input_ids)\n",
    "\n",
    "    # Initialize the decoder state with the encoder state\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    # Initialize the decoder input with the start token\n",
    "    decoder_input = tf.expand_dims([tokenizer.cls_token_id] * input_ids.shape[0], 1)\n",
    "\n",
    "    for t in range(1, summary_ids.shape[1]):\n",
    "        # Pass the encoder state and decoder input to the decoder\n",
    "        predictions, decoder_state, attention_weights = decoder(decoder_input, decoder_state)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss += loss_fn(summary_ids[:, t], predictions)\n",
    "\n",
    "        # Concatenate start token with predictions for the next iteration\n",
    "        decoder_input = tf.concat([tf.expand_dims([tokenizer.cls_token_id] * input_ids.shape[0], 1), predictions], axis=1)\n",
    "\n",
    "\n",
    "    # Average the loss over the sequence length\n",
    "    average_loss = loss / int(summary_ids.shape[1])\n",
    "\n",
    "    # Calculate the gradients and update the weights\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tf.gradients(average_loss, trainable_variables)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4c3a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training parameters\n",
    "epochs = 10\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2376b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the encoder and decoder\n",
    "encoder = Encoder(vocab_size, embedding_size, hidden_size)\n",
    "decoder = Decoder(vocab_size, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7fe31e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Define the checkpoint manager\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6c5a135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    }
   ],
   "source": [
    "#Define the training loop\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "823add46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_724\\3261049699.py\", line 77, in train_step  *\n        summary_encodings = tokenizer(summary_texts, truncation=True, padding=True, return_tensors=\"tf\")\n    File \"c:\\Mustafa\\Frands\\Shah the Kela\\NLP Project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2548, in __call__  *\n        encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n    File \"c:\\Mustafa\\Frands\\Shah the Kela\\NLP Project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2606, in _call_one  *\n        raise ValueError(\n\n    ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     batch_input_ids \u001b[39m=\u001b[39m train_input[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m      4\u001b[0m     batch_summary_ids \u001b[39m=\u001b[39m summary_ids[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m----> 6\u001b[0m     train_step(batch_input_ids, batch_summary_ids, encoder, decoder, optimizer, loss_fn)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Evaluate the model on the test data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Mustafa\\Frands\\Shah the Kela\\NLP Project\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevh7nppao.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(input_ids, summary_texts, encoder, decoder, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m      8\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      9\u001b[0m input_ids \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconvert_to_tensor, (ag__\u001b[39m.\u001b[39mld(input_ids),), \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mint32), fscope)\n\u001b[1;32m---> 10\u001b[0m summary_encodings \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tokenizer), (ag__\u001b[39m.\u001b[39;49mld(summary_texts),), \u001b[39mdict\u001b[39;49m(truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m'\u001b[39;49m), fscope)\n\u001b[0;32m     11\u001b[0m summary_ids \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(summary_encodings)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m encoder_state \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(encoder), (ag__\u001b[39m.\u001b[39mld(input_ids),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegp98vlhv.py:76\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     75\u001b[0m encodings \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mencodings\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(text) \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[39m'\u001b[39;49m\u001b[39mencodings\u001b[39;49m\u001b[39m'\u001b[39;49m,), \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_3\u001b[39m():\n\u001b[0;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m (target_encodings,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegp98vlhv.py:70\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.if_body_2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     69\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_in_target_context_manager), if_body_1, else_body_1, get_state_1, set_state_1, (), \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m encodings \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_call_one, (), \u001b[39mdict\u001b[39;49m(text\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(text), text_pair\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(text_pair), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(all_kwargs)), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefbg6ejsy.py:135\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body_5\u001b[39m():\n\u001b[0;32m    134\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mnot_(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(_is_valid_text_input), (ag__\u001b[39m.\u001b[39;49mld(text),), \u001b[39mNone\u001b[39;49;00m, fscope)), if_body_5, else_body_5, get_state_5, set_state_5, (), \u001b[39m0\u001b[39;49m)\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_6\u001b[39m():\n\u001b[0;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefbg6ejsy.py:131\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___call_one.<locals>.if_body_5\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body_5\u001b[39m():\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mValueError\u001b[39;00m), (\u001b[39m'\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_724\\3261049699.py\", line 77, in train_step  *\n        summary_encodings = tokenizer(summary_texts, truncation=True, padding=True, return_tensors=\"tf\")\n    File \"c:\\Mustafa\\Frands\\Shah the Kela\\NLP Project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2548, in __call__  *\n        encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n    File \"c:\\Mustafa\\Frands\\Shah the Kela\\NLP Project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2606, in _call_one  *\n        raise ValueError(\n\n    ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(train_input), batch_size):\n",
    "        batch_input_ids = train_input[i:i+batch_size]\n",
    "        batch_summary_ids = summary_ids[i:i+batch_size]\n",
    "\n",
    "        train_step(batch_input_ids, batch_summary_ids, encoder, decoder, optimizer, loss_fn)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss = 0\n",
    "        for i in range(0, len(test_input), batch_size):\n",
    "            batch_input_ids = test_input[i:i+batch_size]\n",
    "            batch_summary_ids = test_summary_ids[i:i+batch_size]\n",
    "\n",
    "            # Encode the input\n",
    "            encoder_state = encoder(batch_input_ids)\n",
    "\n",
    "            # Initialize the decoder state with the encoder state\n",
    "            decoder_state = encoder_state\n",
    "\n",
    "            # Initialize the decoder input with the start token\n",
    "            decoder_input = tf.expand_dims([tokenizer.cls_token_id] * batch_size, 1)\n",
    "\n",
    "            # Teacher forcing - feed the target as the next input\n",
    "            for t in range(1, batch_summary_ids.shape[1]):\n",
    "                # Pass the encoder state and decoder input to the decoder\n",
    "                predictions, decoder_state, attention_weights = decoder(decoder_input, decoder_state)\n",
    "\n",
    "                # Calculate the loss\n",
    "                test_loss += loss_fn(batch_summary_ids[:, t], predictions)\n",
    "\n",
    "                # Use teacher forcing\n",
    "                decoder_input = tf.expand_dims(batch_summary_ids[:, t], 1)\n",
    "\n",
    "        # Average the loss over the sequence length\n",
    "        test_loss /= int(test_summary_ids.shape[1])\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {average_loss}, Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    # Loop through the training data\n",
    "    for i in range(0, len(train_input_ids), batch_size):\n",
    "        input_ids_batch = train_input_ids[i:i+batch_size]\n",
    "        summary_ids_batch = train_summary_ids[i:i+batch_size]\n",
    "\n",
    "        # Call the train_step function\n",
    "        train_step(input_ids_batch, summary_ids_batch, encoder, decoder, optimizer, loss_fn)\n",
    "\n",
    "    # Print the loss for this epoch\n",
    "    print(f'Training loss: {average_loss.result()}')\n",
    "\n",
    "    # Reset the average loss for the next epoch\n",
    "    average_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "def generate_summary(input_text, encoder, decoder, max_summary_length=64):\n",
    "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    encoder_state = encoder(input_ids)\n",
    "\n",
    "    decoder_input = tf.expand_dims([tokenizer.cls_token_id], 0)\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for i in range(max_summary_length):\n",
    "        predictions, decoder_state, attention_weights = decoder(decoder_input, decoder_state)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        summary.append(predicted_id)\n",
    "\n",
    "        if predicted_id == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    summary_text = tokenizer.decode(summary, skip_special_tokens=True)\n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = rouge.get_scores(predicted_summary, reference_summary)\n",
    "\n",
    "# Print the F1 score\n",
    "f1_score = scores[0]['rouge-1']['f']\n",
    "print('ROUGE-1 F1 score:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a13e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_ids, encoder, decoder, beam_size=3, max_summary_length=64):\n",
    "    # Encode the input\n",
    "    encoder_state = encoder(input_ids)\n",
    "\n",
    "    # Initialize the beam\n",
    "    beam = [(tf.expand_dims([tokenizer.cls_token_id], 0), 0.0, encoder_state)]\n",
    "\n",
    "    # Generate the summary\n",
    "    for i in range(max_summary_length):\n",
    "        new_beam = []\n",
    "\n",
    "        for sequence, score, decoder_state in beam:\n",
    "            # Get the last word of the sequence\n",
    "            last_word = sequence[:, -1]\n",
    "\n",
    "            # If the last word is the end token, add the sequence to the new beam\n",
    "            if last_word == tokenizer.sep_token_id:\n",
    "                new_beam.append((sequence, score, decoder_state))\n",
    "                continue\n",
    "\n",
    "            # Pass the decoder input and state to the decoder\n",
    "            predictions, decoder_state, attention_weights = decoder(last_word, decoder_state)\n",
    "\n",
    "            # Get the k most likely words\n",
    "            top_k = tf.math.top_k(predictions, k=beam_size)\n",
    "\n",
    "            # Add each new sequence to the new beam\n",
    "            for j in range(beam_size):\n",
    "                word = top_k.indices[0, j].numpy()\n",
    "                prob = top_k.values[0, j].numpy()\n",
    "                new_sequence = tf.concat([sequence, tf.expand_dims([word], 0)], axis=-1)\n",
    "                new_score = score + prob\n",
    "                new_beam.append((new_sequence, new_score, decoder_state))\n",
    "\n",
    "        # Keep only the k best sequences\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "    # Return the best sequence\n",
    "    return beam[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text='...'\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='tf')\n",
    "\n",
    "summary_ids = beam_search(input_ids, encoder, decoder)\n",
    "summary = tokenizer.decode(summary_ids.numpy()[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
